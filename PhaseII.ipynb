{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ4Xt1uf1LT7"
   },
   "source": [
    "# Final Project Phase 2 Summary\n",
    "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 2 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
    "\n",
    "Note: To edit a Markdown cell, double-click on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9oVjpRDswQ",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Data Collection and Cleaning\n",
    "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
    "\n",
    "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "## Data Sources\n",
    "Include sources (as links) to your datasets. Add any additional data sources if needed. Clearly indicate if a data source is different from one submitted in your Phase I, as we will check that it satisfies the requirements.\n",
    "*   Downloaded Dataset Source: https://simfin.com/data/bulk\n",
    "*   Web Collection #1 Source: CHANGED https://www.barchart.com/etfs-funds/performance/percent-change/advances?timeFrame=10y\n",
    "*   Web Collection #2 Source: https://data.nasdaq.com/api/v3/datasets/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj",
    "tags": []
   },
   "source": [
    "## Downloaded Dataset Requirement\n",
    "\n",
    "Fill in the predefined functions with your data scraping/parsing code. You may modify/rename each function as you seem fit, but you must provide at least 3 separate functions that clean each of your required datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium \n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "def data_parser():\n",
    "    df = pd.read_csv('us-income-quarterly 2.csv', delimiter = ';')\n",
    "    df = df.drop(['Depreciation & Amortization','SimFinId','Currency', 'Report Date', 'Publish Date', 'Restated Date', 'Abnormal Gains (Losses)', 'Net Extraordinary Gains (Losses)'], axis=1)\n",
    "    df = df.fillna(0)\n",
    "    data = {}\n",
    "    fiscal_periods = {'Q1': .1, 'Q2': .2, 'Q3': .3, 'Q4': .4}\n",
    "    for i,j in df.groupby('Ticker', as_index=False):\n",
    "        data[i] = j.set_index(j['Fiscal Year'].values + [fiscal_periods[i] for i in j['Fiscal Period'].values]).drop(['Fiscal Year', 'Fiscal Period'], axis=1)\n",
    "        data[i] = data[i].to_json()\n",
    "    data['Tickers'] = list(data.keys())\n",
    "    json.dump(data, open('corporate.json', 'w'))\n",
    "    pprint(data)\n",
    "    return 'Complete'\n",
    "############ Function Call ############\n",
    "data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection Requirement \\#1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXwpJObDFiWM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: THE URL HAS CHANGED FOR THIS SECTION\n",
    "#The links I will now be scraping are the following: \n",
    "#https://www.barchart.com/etfs-funds/performance/percent-change/advances?timeFrame=10y\n",
    "#https://www.barchart.com/etfs-funds/performance/percent-change/declines?timeFrame=10y&orderBy=percentChange&orderDir=asc\n",
    "#Each generate seperate DataFrames which I will use for analysis\n",
    "#Note- the method of scraping both these websites are the same, only the link changes. Therefore I am not creating two different methods.\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from selenium import webdriver\n",
    "import re\n",
    "def web_parser1():\n",
    "    url= \"https://www.barchart.com/etfs-funds/performance/percent-change/declines?timeFrame=10y&orderBy=percentChange&orderDir=asc\"\n",
    "    title_name =re.findall('https://www.barchart.com/etfs-funds/performance/(.+)\\?', url)[0].replace('/', ' ').title()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    df = pd.read_html(html)\n",
    "    df = df[-1].set_index('Symbol').drop(['Time','Links'], axis=1)\n",
    "    df.to_csv(f'{title_name}.csv')\n",
    "    pprint(df)\n",
    "    return \"Complete\"\n",
    "    \n",
    "\n",
    "############ Function Call ############\n",
    "web_parser1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection Requirement \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAkUOqMgXQJG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def web_parser2():\n",
    "    cpi = rq.get(\"https://data.nasdaq.com/api/v3/datasets/RATEINF/CPI_USA/data.json?api_key=gnYkCy78FiQnCZKx7h9z\").json()\n",
    "    df = pd.DataFrame(cpi['dataset_data']['data']).set_index(0).iloc[::-1]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['Log % Change'] = np.log(df[1]).pct_change().multiply(100)\n",
    "    df['Log Change'] = np.log(df[1]).diff(1)\n",
    "    df['Quarterly Rolling Log Change'] = df['Log Change'].rolling(3).mean()\n",
    "    df['Yearly Rolling Log Change'] = df['Log Change'].rolling(6).mean()\n",
    "    df = df[1044:]\n",
    "    df.to_csv('cpi.csv')\n",
    "    pprint(df)\n",
    "    return \"Complete\"\n",
    "\n",
    "############ Function Call ############\n",
    "web_parser2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezovwa1tp0we"
   },
   "source": [
    "## Additional Dataset Parsing/Cleaning Functions\n",
    "\n",
    "Write any supplemental (optional) functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4-s72RNuKLR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "def nasdaq_scrape_ustreasury(dataset):\n",
    "    df = pd.DataFrame(rq.get(f\"https://data.nasdaq.com/api/v3/datasets/{dataset}/data.json?api_key=gnYkCy78FiQnCZKx7h9z\").json()['dataset_data']['data'], columns=rq.get(f\"https://data.nasdaq.com/api/v3/datasets/{dataset}/data.json?api_key=gnYkCy78FiQnCZKx7h9z\").json()['dataset_data']['column_names']).set_index('Date').iloc[::-1]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "def nasdaq_scrape_yale(dataset):\n",
    "    df = pd.DataFrame(rq.get(f\"https://data.nasdaq.com/api/v3/datasets/{dataset}/data.json?api_key=gnYkCy78FiQnCZKx7h9z\").json()['dataset_data']['data'], columns=rq.get(f\"https://data.nasdaq.com/api/v3/datasets/{dataset}/data.json?api_key=gnYkCy78FiQnCZKx7h9z\").json()['dataset_data']['column_names']).set_index('Year').iloc[::-1]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "def noise_filter(data):\n",
    "    n = 30\n",
    "    b = [1.0 / n] * n\n",
    "    a = 1\n",
    "    noiseless= pd.DataFrame(lfilter(b,a,data))\n",
    "    noiseless.index = data.index\n",
    "    return noiseless.dropna()\n",
    "\n",
    "def extra_source1():\n",
    "    real_yield = nasdaq_scrape_ustreasury('USTREASURY/REALYIELD')\n",
    "    real_yield = real_yield.drop('30 YR', axis=1)\n",
    "    real_yield = real_yield.loc[real_yield['20 YR'].dropna().index[0]:]\n",
    "    pprint(real_yield)\n",
    "    real_yield.to_csv('real_yield.csv')\n",
    "    bill_rates = nasdaq_scrape_ustreasury('USTREASURY/BILLRATES')\n",
    "    bill_rates = bill_rates.drop(['52 Wk Coupon Equiv', '8 Wk Bank Discount Rate', '8 Wk Coupon Equiv', '52 Wk Bank Discount Rate' ], axis=1)\n",
    "    bill_rates.to_csv('bill_rates.csv')\n",
    "    pprint(bill_rates)\n",
    "    spy_composite = nasdaq_scrape_yale('YALE/SPCOMP').fillna(0)\n",
    "    spy_composite.to_csv('spy_composite')\n",
    "    pprint(spy_composite)\n",
    "    spy_confidence_individual = noise_filter(np.log(nasdaq_scrape_ustreasury('YALE/US_CONF_INDEX_VAL_INDIV'))[6:].pct_change().rolling(3).mean())\n",
    "    spy_confidence_individual.to_csv('spy_confidence_individual.csv')\n",
    "    pprint(spy_confidence_individual)\n",
    "    spy_confidence_institutional = noise_filter(np.log(nasdaq_scrape_ustreasury('YALE/US_CONF_INDEX_VAL_INST'))[22:].pct_change().rolling(3).mean())\n",
    "    spy_confidence_institutional.to_csv('spy_confidence_institutional')\n",
    "    pprint(spy_confidence_institutional)\n",
    "    spy_crash_individual = noise_filter(np.log(nasdaq_scrape_ustreasury('YALE/US_CONF_INDEX_CRASH_INDIV'))[3:].pct_change().rolling(3).mean())\n",
    "    spy_crash_individual.to_csv('spy_crash_individual.csv')\n",
    "    pprint(spy_crash_individual)\n",
    "    spy_crash_institutional = noise_filter(np.log(nasdaq_scrape_ustreasury('YALE/US_CONF_INDEX_CRASH_INST'))[21:].pct_change().rolling(3).mean())\n",
    "    spy_crash_institutional.to_csv('spy_crash_institutional.csv')\n",
    "    pprint(spy_crash_institutional)\n",
    "    housing_pricing_index = noise_filter(np.log(nasdaq_scrape_ustreasury('YALE/NHPI'))[627:].diff())\n",
    "    housing_pricing_index.to_csv('housing_pricing_index.csv')\n",
    "    pprint(housing_pricing_index)\n",
    "    spy_dividend = noise_filter(np.log(nasdaq_scrape_ustreasury('MULTPL/SP500_DIV_YIELD_MONTH')[1548:]).pct_change().fillna(0))\n",
    "    spy_pe_ratio = noise_filter(nasdaq_scrape_ustreasury('MULTPL/SP500_PE_RATIO_MONTH')[1548:].pct_change().fillna(0))\n",
    "    schiller_pe_ratio =noise_filter(np.log(nasdaq_scrape_ustreasury('MULTPL/SHILLER_PE_RATIO_MONTH')[1548:]).pct_change().fillna(0))\n",
    "    spy_monthly_inflation = noise_filter(np.log(nasdaq_scrape_ustreasury('MULTPL/SP500_INFLADJ_MONTH')[1548:]).pct_change().fillna(0))\n",
    "    multpl = pd.concat([spy_dividend, spy_pe_ratio, schiller_pe_ratio, spy_monthly_inflation], axis=1, keys = ['spy_dividend', 'spy_pe_ratio', 'schiller_pe_ratio', 'spy_monthly_inflation']).fillna(0)\n",
    "    multpl.to_csv('multpl.csv')\n",
    "    pprint(multpl)\n",
    "    return 'Complete'\n",
    "\n",
    "    \n",
    "############ Function Call ############\n",
    "extra_source1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "#Inconsistencies\n",
    "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
    "\n",
    "1. NaN and missing data- nearly every dataset used in this projection contains missing data. There are two ways to handle this- dropping data or filling data. Determining which option is better can only be determined on a case byc ase basis, however the general note used throughout this project was to drop values for data that is non-stationary (will explain later) for analysis and use fillna(0) for stationary datasets.  \n",
    "\n",
    "2. Uneven Indices- Despite coming from the same dataset or api, most of these datasets have different indexes. There were two steps to this problem. First, the \"Date\" or \"Year\" values needed to be converted into DateTime objects so that they can be set as indices. Second, when creating one dataframe the indices needed to start and end at the same index. Therefore, this project required manually going through each of the datasets and locating a range of dates where each dataset had proper values. This is why some datasets have [n:] at the end. Since pandas does not handle ragged DataFrames, this was necessary during preprocessing. \n",
    "\n",
    "3. Unequal Index Intervals- Some datasets (predominantly those from USTreasury) contained indices with uneven intervals. I.E there were only 4 data points from 1960-2000 but 700+ datapoints from 2000-present in the Time Series Data. These unconsistant intervals posed a problem in that pattenrs and trends can not be extrapolated as the data is not formatted correctly. The main solution is to identify these instances and truncate the time series data. \n",
    "\n",
    "4. Stationarity- Most (if not all) of the data is pretty useless preprocessed. That is because none of the time series data is stationary. \"A stationary process has the property that the mean, variance and autocorrelation structure do not change over time.\" (Engineering Statistics Handbook). For time series analysis stationarity is crucial as forecasting and trends can not be identified if data is not stationary. While there are any methods to make data stationary, three methods where implemented in this project. First, converting data to logarithmic data using np.log can make data stationary. Second, converting the time series model to a difference-model such that each value is the change in the discrete intervals bounds the time series model to a certain mean (generally zero). This can be done using .diff() with pandas DataFrames. Lastly, .pct_change() converts DataFrames into the percent change between discrete intervals of the index. The latter method often is the best method as % changes generally reflect seasonal or event-driven patterns. \n",
    "\n",
    "5. Noise- Another property that makes most of this data meaningless at first is noise. Noisy data involves the statistical variation or \"noise\" in data which often renders the general trend moot. Noise within these datasets are reflected by the varying change in daily data which as a results makes the stationary data extremely variable and \"noisy\". There were two methods implemented to reduce noise within the data sets. First, using rolling means (weekly or monthly) smooths the time series data and reduces daily variability. This can be achieved by .rolling(n).mean() for DataFrames. Lastly, scipy.filter provides various signal processing methods to filter noisy data. I used lfilter() to eliminate noise. While I do not fully understand the backend of this method, lfilter can be applied to 1-D data only and is forward filtering based off a filtering function of your choise 'B'. Not knowing much, I used a previous example for this method.  See scipy.signal.lfilter for more details.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseII.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
